{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from collections import Counter\n",
    "import math\n",
    "def init(fileName):\n",
    "    global ws_driver,pos_driver,ner_driver,result,filename\n",
    "\n",
    "    filename = \"./_target/\"+fileName+\".xls\"\n",
    "    result=\"./_target/\"+fileName+\"reslut.xls\"\n",
    "    file = pd.read_excel(filename)\n",
    "\n",
    "    #//print(file.head(1))\n",
    "    data_with_index = file.set_index(\"公開/公告號\")\n",
    "    for i in data_with_index.index:\n",
    "        if('A' in i):\n",
    "            data_with_index=data_with_index.drop(i)\n",
    "    print(data_with_index.head(1))\n",
    "    # * filter of the 公開專利 -> the 公開 one is pretty useless\n",
    "    file =data_with_index\n",
    "    print(len(file))\n",
    "    return file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def run_file (file):\n",
    "    var=file[\"摘要\"]\n",
    "    name=file[\"專利名稱\"]\n",
    "    global sort_key,sort_words,sent_cnt,document\n",
    "    sort_key=[]\n",
    "    sort_words=[]\n",
    "    for i in range (len(file)) :\n",
    "        text =[var[i]]\n",
    "        sent_cnt=0\n",
    "        \n",
    "        self_list=[]## get combine word \n",
    "        common_list=[]## get the most common word \n",
    "        document=[]\n",
    "        # * Run pipeline\n",
    "        # //print(\"Running pipeline ... WS\")\n",
    "        ws = ws_driver(text)\n",
    "        # //print(\"Running pipeline ... POS\")\n",
    "        pos = pos_driver(ws)\n",
    "        # // print(\"Running pipeline ... NER\")\n",
    "        \n",
    "        ner = ner_driver(text)\n",
    "        \n",
    "        print(\"Running pipeline ... done\")\n",
    "        \n",
    "        print()\n",
    "        def pack_ws_pos_sentece(sentence_ws, sentence_pos):\n",
    "            assert len(sentence_ws) == len(sentence_pos)\n",
    "            res = []\n",
    "                \n",
    "            #TODO 字詞的組合 根據屬性表將名詞-> 專有名詞 來增加字詞的特殊性\n",
    "            pre_pos=''\n",
    "            pre_ws=''\n",
    "            for word_ws, word_pos in zip(sentence_ws, sentence_pos):\n",
    "                #! 寫的好爛... 可以透過語言結構做優化\n",
    "                res.append(f\"{word_ws}({word_pos})\")\n",
    "                \n",
    "                if(word_pos==\"Na\" or \"V\" in word_pos):\n",
    "                    common_list.append(word_ws)\n",
    "                    \n",
    "                if((word_pos == \"Na\"or word_pos==\"VC\") and pre_pos!='' ):\n",
    "                # * use \n",
    "                    Nb=f\"{pre_ws}{word_ws}\"\n",
    "                    # // print(Nb)\n",
    "                    self_list.append(pre_ws+word_ws)\n",
    "                    # //res.append(f\"{Nb}(Nb)\")\n",
    "                    pre_pos=\"\"\n",
    "                    pre_ws=\"\"\n",
    "                        \n",
    "                else :\n",
    "                    #* memorizing\n",
    "                    if(word_pos==\"VC\" or word_pos==\"VD\" or word_pos==\"Na\" or word_pos==\"Nb\"):\n",
    "                        pre_pos=word_pos\n",
    "                        pre_ws=word_ws\n",
    "                        \n",
    "                    else :\n",
    "                        pre_pos=''\n",
    "                        pre_ws=''\n",
    "                        # //res.append(f\"{word_ws}({word_pos})\")\n",
    "                if(word_pos==\"PERIODCATEGORY\" or word_pos==\"SEMICOLONCATEGORY\"):\n",
    "                    sent_cnt+1\n",
    "                    #print(word_ws+\"\"+str(sent_cnt))\n",
    "                    \n",
    "                    document.append(res)\n",
    "                \n",
    "            return \"\\u3000\".join(res)\n",
    "        \n",
    "        \n",
    "        # *zip指標的標記\n",
    "        for sentence, sentence_ws, sentence_pos, sentence_ner in zip(text, ws, pos, ner):\n",
    "            # //print(pos)\n",
    "            pack_ws_pos_sentece(sentence_ws, sentence_pos)\n",
    "        sort_key.append(Counter(self_list).most_common(20))\n",
    "\n",
    "        sort_words.append(Counter(common_list).most_common(20))\n",
    "        print(name[i])\n",
    "        print(\"done\")\n",
    "        # //print(sentence)\n",
    "        print(\"------------\")\n",
    "\n",
    "    # ! Suppose to do tf-dlf \n",
    "        # print(sort_key[i])\n",
    "        # print(\"------------\")\n",
    "        # print(sort_words[i])\n",
    "        # for words in sort_key:\n",
    "        #     _TF=(words[1]/len(pos)) \n",
    "        #     _IDF=math.log(words[1]/sent_cnt)\n",
    "        #     print(words[0])\n",
    "        #     print(_TF*_IDF)\n",
    "        #     print()\n",
    "        #  print(+str(TF_IDF))\n",
    "\n",
    "        # *for entity in sentence_ner:\n",
    "        #//     print(entity)\n",
    "        print()\n",
    "        del text\n",
    "        del self_list,common_list,document \n",
    "    return "
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def file_done(file):\n",
    "    #* date format\n",
    "    ori_time=file[\"申請日\"]\n",
    "    date=[]\n",
    "    for i in ori_time:\n",
    "        s=str(i)\n",
    "        date.append(s[0:4]+'/'+s[4:6]+'/'+s[6:8])\n",
    "        #//(datetime(year=int(), month=int(s[4:6]), day=int(s[6:8])))\n",
    "    #//print((date))\n",
    "\n",
    "    #* export back to the file \n",
    "\n",
    "    print(len(TF_IDF))\n",
    "\n",
    "    bank ={\n",
    "            # \"ID\":file.index,\n",
    "            \"申請日\":date,\n",
    "            \"名稱\":file[\"專利名稱\"],\n",
    "            \"申請人\":file[\"申請人\"],\n",
    "            \"IPC\":file[\"IPC\"],\n",
    "            \"引用專利\":file[\"引用專利\"],\n",
    "            \"被參考次數\":file[\"被參考次數\"],    \n",
    "            \"專利摘要\":file[\"摘要\"],\n",
    "            \"摘要關鍵字\":sort_key,\n",
    "            \"摘要常用字\":sort_words,\n",
    "            \"TF_IDF\":TF_IDF,\n",
    "            }\n",
    "    print (pd.DataFrame(bank))\n",
    "    pd.DataFrame(bank).drop_duplicates(subset =\"名稱\",keep = False, inplace = True)\n",
    "    pd.DataFrame(bank).to_excel(result)\n",
    "    return \n",
    "#// bank ={\"名稱\":name,\n",
    "#//         \"KEY\":sort_key,\n",
    "#//         \"WORD\":sort_words\n",
    "#//         }\n",
    "#// print (pd.DataFrame(bank))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Initializing drivers ... WS\")\n",
    "ws_driver = CkipWordSegmenter(level=2)\n",
    "print(\"Initializing drivers ... POS\")\n",
    "pos_driver = CkipPosTagger(level=2)\n",
    "print(\"Initializing drivers ... NER\")\n",
    "ner_driver = CkipNerChunker(level=2)\n",
    "print(\"Initializing drivers ... done\")\n",
    "print()\n",
    "#for i in {\"中信金控\",\"兆豐金控\",\"台新金控\",\"台灣金融控股\",\"國泰金控\",\"第一金控\"}:\n",
    "for i in {\"新光金控\"}:\n",
    "    file=init(i)\n",
    "    run_file(file)\n",
    "    file_done(file)\n",
    "    print(i+\" is done\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}